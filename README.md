# HIV DRM detection with machine learning

## Prerequisites
To be able to run this pipeline several steps are needed. 
### dependencies
for this pipeline you will need `python >= 3.6`, `snakemake>=5.26.1` and the packages specified in `utils_hiv/requirements.txt`. To install the necessary package in a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) virtual environment run:  
```shell
$ cd /path/to/this/directory
$ conda create -n pipelineDRMs python=3.7 snakemake">=5.26.1" -y
$ conda activate pipelineDRMs
$ pip install -e utils_hiv
```

### Alignment data
For this pipeline to run you will need several alignments of HIV-1 pol RT sequences from at least 2 datasets: a training set and at least one testing set.  
Each training/testing set is composed of a FASTA alignment of treatment-naive sequences and another of treatment-experienced sequences.  
To get the positions of each residue w.r.t. the reference HXB2 sequence and get a suitable format for encoding, you should upload each of your alignments `trainNaive.fa`, `trainTreated.fa`, `testNaive.fa` and `testTreated.fa` into [Stanford's HIVdb program](https://hivdb.stanford.edu/hivdb/by-sequences/). For each uploaded alignment you will get the `PrettyRTAA.tsv` and `ResistanceSummary.tsv` files which are need for dataset encoding.  


## pipelines 
### data preprocessing pipeline
This pipeline takes the files generated above by Stanford's HIVdB and encodes them to vectorial form.  
The pipeline takes as input the directory where those files are stored, each dataset you want to encode must be in a separate subdirectory. The pipeline also needs a directory where metadata files are located.  
In our example we want to encode 2 datasets, a UK dataset and an African dataset, so our directory and files should look like this:   

````
.
├── data_dir
│   ├── Africa
│   │   ├── PrettyRT_naive.tsv
│   │   ├── PrettyRT_treated.tsv
│   │   ├── ResistanceSummary_naive.tsv
│   │   └── ResistanceSummary_treated.tsv
│   └── UK
│       ├── PrettyRT_naive.tsv
│       ├── PrettyRT_treated.tsv
│       ├── ResistanceSummary_naive.tsv
│       └── ResistanceSummary_treated.tsv
└── metadata_dir
    ├── Africa-metadata.tsv
    └── UK-metadata.tsv
````
The pipeline looks as follows, with the `process_data` rule encoding sequences as binary vectors of mutation presence/absence, and the `homogenize_data` rule making sure all encoded datasets have the same set of features so that classifiers trained on one dataset can predict labels for another.  

<p align="center">
    <img src="images/graphPreprocess.svg" alt="data preprocessing pipeline execution graph">
</p>

### main training pipeline
This pipeline trains the classifiers on a training set and gets predictions on a testing set. The inputs are specified in the `config.yaml` configuration file. The input data is the one generated by the preprocessing pipeline. 

<p align="center">
    <img src="images/graphMain.svg" alt="main pipeline execution graph">
</p>